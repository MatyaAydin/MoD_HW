{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVcEtGJ8QeYP"
   },
   "source": [
    "# 3. Sharpness-aware minimization (SAM) wirh Sparse Networks - 25 points\n",
    "\n",
    "## 3.1 Get a sparse networks through pruning\n",
    "**Pruning** is a technique used to reduce the size and complexity of a neural network model by removing **(setting to zero)** less important parameters. The goal is to create a more efficient model that retains its predictive accuracy while being smaller, which can improve both inference speed and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilWbesreILh4"
   },
   "source": [
    "Let's train a simple model on the MNIST dataset to learn about pruning at first. We just use 10% of the dataset for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJta4OIBcEpy"
   },
   "source": [
    "### 3.1.1 Train a dense network with SGD\n",
    "Let us first train a dense model with SGD. We reuse the model for the discriminator of the GAN in Homework 2 and name it 'Classifier'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXSZdsCILh4"
   },
   "outputs": [],
   "source": [
    "from lib.part3.utils import *\n",
    "max_epochs = 10\n",
    "device = \"cpu\" # Change this if you can and want to use a GPU device\n",
    "model = Classifier().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLI-Alb-ILh5"
   },
   "source": [
    "We define the optimizing process of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXZG0iPPILh5"
   },
   "outputs": [],
   "source": [
    "def optimize_sgd(model, optimizer, img, label):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udzDwcQqX-GE"
   },
   "source": [
    "The following cell runs the training loop, this might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y_p6FbG0ILh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.933 accuracy on the validation set.\n",
      "Epoch 1 with 0.954 accuracy on the validation set.\n",
      "Epoch 2 with 0.957 accuracy on the validation set.\n",
      "Epoch 3 with 0.962 accuracy on the validation set.\n",
      "Epoch 4 with 0.963 accuracy on the validation set.\n",
      "Epoch 5 with 0.967 accuracy on the validation set.\n",
      "Epoch 6 with 0.967 accuracy on the validation set.\n",
      "Epoch 7 with 0.966 accuracy on the validation set.\n",
      "Epoch 8 with 0.966 accuracy on the validation set.\n",
      "Epoch 9 with 0.966 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, optimize_sgd, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnvFT-DxILh6"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LKTkbxGyILh6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9778 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgVCA_jwJbPx"
   },
   "source": [
    "### 3.1.2 Sparse network with magnitude-based pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubynCgs2b0wT"
   },
   "source": [
    "Magnitude-based pruning specifically focuses on **removing weights that have the smallest absolute values**, under the assumption that weights with smaller magnitudes contribute less to the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkjQPvAXcZcI"
   },
   "source": [
    "**(1)** (6 points) Realize magnitude-based pruning below, which removes a part of weights that have the smallest absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xfb19GShJZFI"
   },
   "outputs": [],
   "source": [
    "def magnitude_prune(model, prune_fraction):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            # FILL: Get weight's absolute values\n",
    "            weight_abs = param.abs()\n",
    "            # FILL: Compute the threshold (s.t prune_fraction % of the weights with the lowest abs are removed)\n",
    "            threshold = torch.kthvalue(weight_abs.flatten(), int(prune_fraction * weight_abs.numel()))[0]\n",
    "            # FILL: Prune weights below the threshold\n",
    "            with torch.no_grad():\n",
    "                param[weight_abs < threshold] = 0\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fr0VzGC_NPtv"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# Copy a model for pruning\n",
    "sparse_model = copy.deepcopy(model)\n",
    "# Get a sparse model by pruning 50% parameters\n",
    "sparse_model = magnitude_prune(sparse_model, prune_fraction=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sOOCdBoRgzL"
   },
   "source": [
    "Copy a sparse model for SAM implementation later in 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Xzi_XqlmRbC4"
   },
   "outputs": [],
   "source": [
    "sparse_model_sam = copy.deepcopy(sparse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_EFs4FiQh1X"
   },
   "source": [
    "Evaluation after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4ZtXqwuaPtNF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9386 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz5gPbMuGiAW"
   },
   "source": [
    "### 3.1.3 Finetune the sparse model\n",
    "\n",
    "We finetune the sparse model after pruning with SGD to recover its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jD0tZKllNi00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.942 accuracy on the validation set.\n",
      "Epoch 1 with 0.942 accuracy on the validation set.\n",
      "Epoch 2 with 0.942 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "finetune_epoch = 3\n",
    "train_model(sparse_model, optimizer, optimize_sgd, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rha2SQXBQnV8"
   },
   "source": [
    "Evaluate the sparse model after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sby9YSR9PrYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9543 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHZY8_nZaMO-"
   },
   "source": [
    "**(2)** (2 point) What are the pros and cons of sparse networks?\n",
    "\n",
    "**Pros:**\n",
    "* Faster training\n",
    "* Faster evaluation\n",
    "* Keeping the weights associated to fewer features only could lead to more explainability and/or better feature selection, just like an $\\ell_1$ regularizer that forces sparcity (espiecally at smaller scale)\n",
    "\n",
    "**Cons:**\n",
    "* Lower accuracy\n",
    "* Need to finetune afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97IxkV9Rl5bT"
   },
   "source": [
    "## 3.2 Train the sparse model with SAM\n",
    "\n",
    "Sharpness-aware minimization (SAM) is a new optimization technique, which is satisfied with not just a low loss, instead it seeks a neighborhood with uniformly low loss. SAM is motivated by the link between the geometry of the loss landscape and generalization. It makes sense that a low loss within a uniformly low loss neighborhood will generalize better than a low loss within a region of higher variance.\n",
    "\n",
    "To be specific, we consider a model with the weight vector of $\\mathbf{w}$ and the training loss $L_S$. SAM aims to minimize the maximum loss within a small region which is usually a $\\ell_2$ ball with $\\rho$ radius. Note that $\\rho$ is a small value close to $0$. Therefore, SAM can be formulated as a minimax optimization problem:\n",
    "$$\\min_{\\mathbf{w}} \\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2\\leq \\rho} L_S (\\mathbf{w} + \\mathbf{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFY_FOCEILh3"
   },
   "source": [
    "**(3)** (3 points) Please solve the inner maximum problem by first-order Taylor expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u8vP9FCtTuh"
   },
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here\n",
    "\n",
    "With the first-order Taylor expansion, the objective function becomes:\n",
    "\n",
    "$$L_S (\\mathbf{w} + \\mathbf{\\epsilon}) \\approx L_S (\\mathbf{w}) + \\epsilon^{\\top} \\nabla L_S (\\mathbf{w})$$\n",
    "\n",
    "Which leads to the following inner maximum problem:\n",
    "\n",
    "$$\\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2\\leq \\rho} \\epsilon^{\\top} \\nabla L_S (\\mathbf{w})$$\n",
    "\n",
    "As the objective is linear, we know that the optimum will be on the boundary:\n",
    "\n",
    "$$\\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2 = \\rho} \\epsilon^{\\top} \\nabla L_S (\\mathbf{w})$$\n",
    "\n",
    "Using $\\|\\mathbf{\\epsilon}\\|^2_2 = \\rho^2$ as an equivalent constraint that simplifies computations, this problem can easily be solved with Lagrange multipliers and leads to the following 2 by 2 system of equations:\n",
    "\n",
    "$$\\nabla_{\\epsilon}(\\epsilon^{\\top} \\nabla_{\\mathbf{w}} L_S (\\mathbf{w})) = \\nabla_{\\mathbf{w}} L_S (\\mathbf{w}) = \\lambda \\nabla_{\\epsilon} (\\|\\mathbf{\\epsilon}\\|^2_2 - \\rho^2) = 2 \\epsilon$$\n",
    "$$\\|\\mathbf{\\epsilon}\\|^2_2 = \\rho^2$$\n",
    "\n",
    "By substitution, we get:\n",
    "\n",
    "$$\\epsilon^{\\star} = \\rho \\frac{L_S (\\mathbf{w})}{\\| \\nabla L_S (\\mathbf{w})\\|_2}$$\n",
    "\n",
    "Which is indeed the most aligned vector with $\\nabla L_S (\\mathbf{w})$ that has a norm equal to $\\rho$, resulting in a maximization of the scalar product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwbpa28BILh6"
   },
   "source": [
    "**(4)** (8 points) Now we will train the same model using the SAM optimizer.\n",
    "Please implement SAM by the two steps below. The first step is for the maximizer which calculates $\\epsilon$ obtained in question (1). The second step is the normal step for the minimizer: $\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta_t \\nabla L_S (\\mathbf{w}_t + \\mathbf{\\epsilon}_t)$ where $\\eta_t$ is step size. Note that we set $\\rho=0.05$.\n",
    "\n",
    "Hint: be careful about weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7mZeWh05ILh6"
   },
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, lr=0.01, rho=0.05):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, lr)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # Note that p.grad gets the gradient; p.data gets the weight.\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        norm += 1e-12 # Avoid zero norm\n",
    "        return norm\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self):\n",
    "        # Add the perturbation on the weight.\n",
    "        # Hint: the norm of the gradient can be calculated by _grad_norm() function.\n",
    "        # Hint: self.param_groups to get access to the weight and gradient of each parameter\n",
    "        # FILL\n",
    "\n",
    "        rho = self.param_groups[0]['rho']\n",
    "        grad_norm = self._grad_norm()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    epsilon_t = rho*p.grad/(grad_norm)\n",
    "                    p.add_(epsilon_t)\n",
    "        \n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        # FILL\n",
    "        # Hint: Remember to change the parameters back #retirer perturbation ?\n",
    "\n",
    "        rho = self.param_groups[0]['rho']\n",
    "        grad_norm = self._grad_norm()\n",
    "\n",
    "        lr = self.param_groups[0]['lr']\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    grad = p.grad\n",
    "                    #tjrs le bon epsilon ou faut le return dans first step ?\n",
    "                    epsilon_t = rho*grad/(grad_norm)\n",
    "                    p.add_(-lr*grad - epsilon_t)\n",
    "\n",
    "\n",
    "\n",
    "        self.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F6EaFSWILh6"
   },
   "source": [
    "Define an optimizer of `SAM` for the model. We recommend using `SGD` as base optimizer with a learning rate of $0.05$ (which is same with SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "f3D9IZGql74k"
   },
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.SGD\n",
    "sam_optimizer = SAM(sparse_model_sam.parameters(), base_optimizer, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwYNhqGoILh6"
   },
   "source": [
    "**(5)** (4 points) Please define the optimizing process of SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "q9-zRaSVCwGO"
   },
   "outputs": [],
   "source": [
    "def optimize_sam(model, optimizer, img, label):\n",
    "\n",
    "    enable_running_stats(model)\n",
    "    # First forward-backward pass\n",
    "    # FILL\n",
    "    # Hint: use sam_optimizer above\n",
    "    # Hint: use loss 'cross_entropy'\n",
    "    pred = model(img)\n",
    "    loss = cross_entropy(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.first_step()\n",
    "\n",
    "    disable_running_stats(model)\n",
    "    # Second forward-backward pass\n",
    "    # FILL\n",
    "    pred = model(img)\n",
    "    loss = cross_entropy(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.second_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "45KsL4m4DAU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.966 accuracy on the validation set.\n",
      "Epoch 1 with 0.969 accuracy on the validation set.\n",
      "Epoch 2 with 0.973 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(sparse_model_sam, sam_optimizer, optimize_sam, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSh_GOxoNE-"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the sparse model finetuned with SAM on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3IAfT8aMoNWb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9758 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model_sam)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi2NLImh_QeD"
   },
   "source": [
    "**(6)** (2 points) Give a conclusion comparing SAM with SGD. Is there any drawback of SAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "Write your answer here\n",
    "\n",
    "Training with SAM allows us to get better accuracy. We almost recover the accuracy that we had without pruning.\n",
    "\n",
    "However, SAM takes roughly twice as long to train because it has to do two forward-backward passes for the min and max part of the problem. Finding a suitable $\\rho$ also adds a difficulty to the problem."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
